{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ba5d64",
   "metadata": {},
   "source": [
    "\n",
    "   # Individuals Annual Income in The USA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " ## Abstract:\n",
    "\n",
    " The goal of this project is to build a classification model to accurately predict whether an individual annual income in the U.S is more or less than 50k. The datasets were imported from the U.S Census website (https://census.com/). Different machine learning models, future engineering, and techniques were applied to successfully accomplish our project. \n",
    "\n",
    " ## Design:\n",
    "\n",
    " Starting with performing EDA and data cleaning, we found that our data is imbalanced so a weight adjustment by *3 was necessary to prevent overfitting and a low F1 score. A Knn model was selected as a baseline for it is simplicity and to have a better insight about our data and following procedure.\n",
    "\n",
    "\n",
    " ## Data:\n",
    "\n",
    " The data contain over 280000 rows and more than 40 columns. Most of it was categorical such as work class, education, marital status, gender, race, etc...\n",
    "\n",
    "\n",
    " ## Algorithms:\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "1.\tConverting categorical features to binary dummy variables\n",
    "2.\tConverting the target columns to binary\n",
    "3.\tSquaring the age features helped to improve our data accuracy\n",
    "4.\tTrying different feature engineerings such as addition multiplication and division\n",
    "5.\tUsing stacking\n",
    "\n",
    "### Models\n",
    "\n",
    "* Logistic regression\n",
    "* Logistic Regression 3*1(Balanced by multiplying 1's rows by 3)\n",
    "* Logistic Regression 3*1 age^2 (in addition to balancing age is squared)\n",
    "* stacked (stacking(Logistic regression,Logistic Regression 3*1 age^2,kNN))\n",
    "* kNN\n",
    "* Scaled Logistic Regression\n",
    "* Decision Tree\n",
    "* Extra Trees\n",
    "* Random forest \n",
    "\n",
    "### Model Evaluation and Selection\n",
    "\n",
    " The entire dataset of 280k observation was split into 92/4/4 train/validation/ Test. While examining our results from all models we found that logistic regression is the best in terms of F1, train, validation ROC, and test outcomes.\n",
    "\n",
    "* Final random Logistic regression score: \n",
    "* Training accuracy 0.938\n",
    "* Testing accuracy 0.939\n",
    "* F1 score 0.544\n",
    "* Recall 0.569\n",
    "* Precision 0.569\n",
    "\n",
    " ## Tools:\n",
    "\n",
    " * Jupyter Notebook  \n",
    " * Statsmodels.api  \n",
    " * Seaborn \n",
    " * Statsmodels.formula.api \n",
    " * Matplotlib \n",
    " * Numpy \n",
    " * Pandas \n",
    " * Sklearn \n",
    " * Patsy \n",
    " * Pickle\n",
    "\n",
    " ## Communication:\n",
    "\n",
    " We used pdf and PowerPoint slides to communicate our findings and results.\n",
    "\n",
    "\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0640f17",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
